{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment11-2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1ZLvQVZuvDdUbk48ZNwQtPCN3sBb2Trmi",
      "authorship_tag": "ABX9TyPHmFbzd7BcJ25emCQlzaJm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janghyukjin/MachineLearning/blob/master/Assignment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMoCw28_dTSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd drive/My\\ Drive/machineLearning\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmOPjnh6iYVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from nltk.tag import pos_tag, untag\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(\"~\")\n",
        "review_data = load_files(r\"movie_review\")\n",
        "\n",
        "X, y = review_data.data, review_data.target\n",
        "print(len(X))\n",
        "print(len(y))\n",
        "documents = []\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "print(\"~!\")\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    document = document.split()\n",
        "    document = [word for word in document if len(word)>=3]\n",
        "    tagged_list = pos_tag(document)\n",
        "\n",
        "    document = ' '.join(document)\n",
        "    final_doc = []\n",
        "    print(\"!!\")\n",
        "    for t in tagged_list:\n",
        "      if t[1] in [\"NN\", \"NNP\", \"NNS\", \"NNPS\"]:\n",
        "        tmp = stemmer.lemmatize(t[0], pos=\"n\")\n",
        "        final_doc.append(tmp)\n",
        "      elif t[1] in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
        "        tmp = stemmer.lemmatize(t[0], pos=\"v\")\n",
        "        final_doc.append(tmp)\n",
        "      elif t[1] in [\"JJ\", \"JJR\", \"JJS\"]:\n",
        "        tmp = stemmer.lemmatize(t[0], pos=\"a\")\n",
        "        final_doc.append(tmp)\n",
        "      elif t[1] in [\"RB\", \"RBR\", \"RBS\", \"RP\"]:\n",
        "        tmp = stemmer.lemmatize(t[0], pos=\"r\")\n",
        "        final_doc.append(tmp)\n",
        "      else:\n",
        "        tmp = stemmer.lemmatize(t[0])\n",
        "        final_doc.append(tmp)\n",
        "    final_doc = ' '.join(final_doc)\n",
        "\n",
        "    documents.append(final_doc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-MqSQaFitgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=1200, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "print(len(X))\n",
        "print(len(y))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "v=np.random.randn(50,1200) \n",
        "w=np.random.randn(1,50) \n",
        "m = 1400\n",
        "test_m = 600\n",
        "cost = 0\n",
        "test_cost = 0\n",
        "accuracy = 0\n",
        "test_accuracy = 0\n",
        "cost_list = np.empty(0)\n",
        "accuracy_list = np.empty(0)\n",
        "test_cost_list = np.empty(0)\n",
        "test_accuracy_list = np.empty(0)\n",
        "bias2 = np.zeros((50, 1)) \n",
        "bias3 = np.zeros((1, 1))\n",
        "lamda = 0.5\n",
        "cnt = 0\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBNgT2yQjWCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.5\n",
        "count = 0\n",
        "jx = np.empty(0)\n",
        "final_cnt = 0\n",
        "while(count<20000):\n",
        "  if count%10==0:\n",
        "    print(count)\n",
        "  \n",
        "  jx = np.append(jx, count)\n",
        "  count += 1\n",
        "  \n",
        "  #propagation\n",
        "\n",
        "  bias_z = np.dot(v, X_train.T)\n",
        "  bias_z += bias2 \n",
        "  sig_z = sigmoid(bias_z)\n",
        "  bias_h = np.dot(w, sig_z)\n",
        "  bias_h += bias3\n",
        "  sig_h = sigmoid(bias_h)\n",
        "  \n",
        "  #objective Function\n",
        "  cost = (-1/1400)*np.sum(y_train*np.log(sig_h) + (1-y_train)*np.log(1-sig_h)) + (lamda/(2*1400))*(np.sum(np.square(v)) + np.sum(np.square(w)))\n",
        "  cost_list = np.append(cost_list,cost)\n",
        "  print(\"cost :                 \",cost)\n",
        "  #Calculate Accuracy\n",
        "  accuracy = 0\n",
        "\n",
        "  for i in range(1401):\n",
        "    if sig_h[0][i]>=0.5:\n",
        "      sig_h[0][i] = 1\n",
        "    else:\n",
        "      sig_h[0][i] = 0\n",
        "    if y_train[i] == sig_h[0][i]:\n",
        "      accuracy += 1\n",
        "  accuracy/=1400\n",
        "  accuracy*=100 \n",
        "  print(\"training :       \",accuracy)\n",
        "  accuracy_list = np.append(accuracy_list, accuracy)\n",
        "\n",
        "  #back propagation\n",
        "  delta_h = (sig_h-y_train)\n",
        "  delta_z = np.dot(np.transpose(w),delta_h)*sig_z*(1-sig_z)\n",
        "\n",
        "  delta_bias3 = np.sum(delta_h, axis=1, keepdims=True)/1000\n",
        "  delta_bias2 = np.sum(delta_z, axis=1, keepdims=True)/1000\n",
        "\n",
        "  delta_w = np.dot(delta_h,np.transpose(sig_z))\n",
        "  delta_v = np.dot(delta_z,X_train)\n",
        "  delta_w/=1000\n",
        "  delta_v/=1000\n",
        "  delta_w += (lamda/(1000))*w\n",
        "  delta_v += (lamda/(1000))*v\n",
        "\n",
        "  #update theta, bias\n",
        "  v -= alpha * delta_v \n",
        "  w -= alpha * delta_w \n",
        "  bias2 -= alpha * delta_bias2 \n",
        "  bias3 -= alpha * delta_bias3\n",
        "\n",
        "  #test images propagation\n",
        "\n",
        "  test_bias_z = np.dot(v, X_test.T)\n",
        "  test_bias_z += bias2 \n",
        "  test_sig_z = sigmoid(test_bias_z)\n",
        "  test_bias_h = np.dot(w, test_sig_z)\n",
        "  test_bias_h += bias3\n",
        "  test_sig_h = sigmoid(test_bias_h)\n",
        "  #test images objective function\n",
        "  test_cost = (-1/600)*np.sum(y_test*np.log(test_sig_h) + (1-y_test)*np.log(1-test_sig_h)) + (lamda/(2*600))*(np.sum(np.square(v)) + np.sum(np.square(w)))\n",
        "  test_cost_list = np.append(test_cost_list,test_cost)\n",
        "  #test images accuracy\n",
        "  test_accuracy = 0\n",
        "\n",
        "  for i in range(601):\n",
        "    if test_sig_h[0][i]>=0.5:\n",
        "      test_sig_h[0][i] = 1\n",
        "    else:\n",
        "      test_sig_h[0][i] = 0\n",
        "    if y_test[i] == test_sig_h[0][i]:\n",
        "      test_accuracy += 1\n",
        "  test_accuracy/=600\n",
        "  test_accuracy*=100 \n",
        "  print(\"test :\", test_accuracy)\n",
        "  test_accuracy_list = np.append(test_accuracy_list, test_accuracy)\n",
        "  #change running rate for improving accuracy\n",
        "  if test_accuracy> 77:\n",
        "    alpha = 0.05\n",
        "  elif test_accuracy > 65: \n",
        "    alpha = 0.01\n",
        "\n",
        "  if test_accuracy > 82: \n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeKvAHGKeMj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_train = []\n",
        "y_pred_test = []\n",
        "for s in range(1):\n",
        "\n",
        "    test_bias_z = np.dot(v, X_test.T)\n",
        "    test_bias_z += bias2 \n",
        "    test_sig_z = sigmoid(test_bias_z)\n",
        "    test_bias_h = np.dot(w, test_sig_z)\n",
        "    test_bias_h += bias3\n",
        "    test_sig_h = sigmoid(test_bias_h)\n",
        "    for i in range(1401):\n",
        "      if sig_h[0][i]>=0.5:\n",
        "        sig_h[0][i] = 1\n",
        "      else:\n",
        "        sig_h[0][i] = 0\n",
        "      y_pred_train.append(sig_h[0][i])\n",
        "\n",
        "    for i in range(601):\n",
        "      if test_sig_h[0][i]>=0.5:\n",
        "        test_sig_h[0][i] = 1\n",
        "      else:\n",
        "        test_sig_h[0][i] = 0\n",
        "      y_pred_test.append(test_sig_h[0][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB528hwYdp_x",
        "colab_type": "text"
      },
      "source": [
        "# **Plot the loss curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2XrEctFgfkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_iixWzxdxoZ",
        "colab_type": "text"
      },
      "source": [
        "# **Plot the accuracy curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAM8e56LZyiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrRIwzkLd2Hz",
        "colab_type": "text"
      },
      "source": [
        "# **Plot the quantitative results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cj_426BgbEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa3q5x1EcGrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}