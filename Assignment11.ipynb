{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment11.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "11LhkRtxNttrIIr0Ia5S_LOhzM71XwkfK",
      "authorship_tag": "ABX9TyN9YDZ0PLAAylAXW0OTxco2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janghyukjin/MachineLearning/blob/master/Assignment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HvOUKn0uvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnKkHbnXsdwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "\n",
        "X, y = review_data.data, review_data.target\n",
        "print(len(X))\n",
        "print(len(y))\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "print(len(X))\n",
        "print(len(y))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1knPzcCzVnim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u=np.random.randn(120,1500) \n",
        "v=np.random.randn(10,120) \n",
        "w=np.random.randn(1,10) \n",
        "m = 1400\n",
        "test_m = 600\n",
        "cost = 0\n",
        "test_cost = 0\n",
        "accuracy = 0\n",
        "test_accuracy = 0\n",
        "cost_list = np.empty(0)\n",
        "accuracy_list = np.empty(0)\n",
        "test_cost_list = np.empty(0)\n",
        "test_accuracy_list = np.empty(0)\n",
        "bias1 = np.zeros((120, 1))\n",
        "bias2 = np.zeros((10, 1)) \n",
        "bias3 = np.zeros((1, 1))\n",
        "lamda = 0.5\n",
        "cnt = 0\n",
        "print(y_train)\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "\n",
        "print(len(y_train))\n",
        "print(len(y_test))\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjoiMb4j01YE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.5\n",
        "count = 0 \n",
        "final_cnt = 0\n",
        "jx = np.empty(0)\n",
        "while(count<10000):\n",
        "  if count%10==0:\n",
        "    print(count)\n",
        "  jx = np.append(jx, count)\n",
        "  count += 1\n",
        "  \n",
        "  #propagation\n",
        "\n",
        "  bias_y = np.dot(u, X_train.T)\n",
        "\n",
        "  bias_y += bias1 \n",
        "  sig_y = sigmoid(bias_y)\n",
        "  bias_z = np.dot(v, sig_y)\n",
        "  bias_z += bias2 \n",
        "  sig_z = sigmoid(bias_z)\n",
        "  bias_h = np.dot(w, sig_z)\n",
        "  bias_h += bias3\n",
        "  sig_h = sigmoid(bias_h)\n",
        "  \n",
        "  #objective Function\n",
        "  cost = (-1/1400)*np.sum(y_train*np.log(sig_h) + (1-y_train)*np.log(1-sig_h)) + (lamda/(2*1000))*(np.sum(np.square(u)) + np.sum(np.square(v)) + np.sum(np.square(w)))\n",
        "  cost_list = np.append(cost_list,cost)\n",
        "  \n",
        "  #Calculate Accuracy\n",
        "  accuracy = 0\n",
        "\n",
        "  for i in range(1401):\n",
        "    if sig_h[0][i]>=0.5:\n",
        "      sig_h[0][i] = 1\n",
        "    else:\n",
        "      sig_h[0][i] = 0\n",
        "    if y_train[i] == sig_h[0][i]:\n",
        "      accuracy += 1\n",
        "  accuracy/=1400\n",
        "  accuracy*=100 \n",
        "  accuracy_list = np.append(accuracy_list, accuracy)\n",
        "\n",
        "  #back propagation\n",
        "  delta_h = (sig_h-y_train)\n",
        "  delta_z = np.dot(np.transpose(w),delta_h)*sig_z*(1-sig_z)\n",
        "  delta_y = np.dot(np.transpose(v),delta_z)*sig_y*(1-sig_y)\n",
        "\n",
        "  delta_bias3 = np.sum(delta_h, axis=1, keepdims=True)/1000\n",
        "  delta_bias2 = np.sum(delta_z, axis=1, keepdims=True)/1000\n",
        "  delta_bias1 = np.sum(delta_y, axis=1, keepdims=True)/1000\n",
        "\n",
        "  delta_w = np.dot(delta_h,np.transpose(sig_z))\n",
        "  delta_v = np.dot(delta_z,np.transpose(sig_y))\n",
        "  delta_u = np.dot(delta_y,X_train)\n",
        "  delta_w/=1000\n",
        "  delta_v/=1000\n",
        "  delta_u/=1000\n",
        "  delta_w += (lamda/1000)*w\n",
        "  delta_v += (lamda/1000)*v\n",
        "\n",
        "  delta_u += (lamda/1000)*u\n",
        "\n",
        "  #update theta, bias\n",
        "  u -= alpha * delta_u \n",
        "  v -= alpha * delta_v \n",
        "  w -= alpha * delta_w \n",
        "  bias1 -= alpha * delta_bias1 \n",
        "  bias2 -= alpha * delta_bias2 \n",
        "  bias3 -= alpha * delta_bias3\n",
        "\n",
        "  #test images propagation\n",
        "  test_bias_y = np.dot(u, X_test.T)\n",
        "  test_bias_y += bias1 \n",
        "  test_sig_y = sigmoid(test_bias_y)\n",
        "  test_bias_z = np.dot(v, test_sig_y)\n",
        "  test_bias_z += bias2 \n",
        "  test_sig_z = sigmoid(test_bias_z)\n",
        "  test_bias_h = np.dot(w, test_sig_z)\n",
        "  test_bias_h += bias3\n",
        "  test_sig_h = sigmoid(test_bias_h)\n",
        "  #test images objective function\n",
        "  test_cost = (-1/600)*np.sum(y_test*np.log(test_sig_h) + (1-y_test)*np.log(1-test_sig_h)) + (lamda/(2*9000))*(np.sum(np.square(u)) + np.sum(np.square(v)) + np.sum(np.square(w)))\n",
        "  test_cost_list = np.append(test_cost_list,test_cost)\n",
        "  #test images accuracy\n",
        "  test_accuracy = 0\n",
        "\n",
        "  for i in range(601):\n",
        "    if test_sig_h[0][i]>=0.5:\n",
        "      test_sig_h[0][i] = 1\n",
        "    else:\n",
        "      test_sig_h[0][i] = 0\n",
        "    if y_test[i] == test_sig_h[0][i]:\n",
        "      test_accuracy += 1\n",
        "  test_accuracy/=600\n",
        "  test_accuracy*=100 \n",
        "  print(test_accuracy)\n",
        "  test_accuracy_list = np.append(test_accuracy_list, test_accuracy)\n",
        "  #change running rate for improving accuracy\n",
        "  if test_accuracy > 80: \n",
        "    alpha = 0.8\n",
        "  if test_accuracy > 88.5: \n",
        "    alpha = 0.5\n",
        "  if test_accuracy > 89: \n",
        "    final_cnt += 1\n",
        "  if final_cnt > 50:\n",
        "    break\n",
        "  if final_cnt>5:\n",
        "    if test_accuracy<89:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH56W1D0gYjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}